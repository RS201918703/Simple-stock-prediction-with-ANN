{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "027b28a1",
   "metadata": {},
   "source": [
    "# Predicting stock prices using MLP and LSTM models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f815ec",
   "metadata": {},
   "source": [
    "#### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e142f45c",
   "metadata": {},
   "source": [
    "The task of predicting stock market prices is challenging. Stock prediction is of interest to most investors due to its high volatility. Even now, some investors use a combination of technical and fundamental analysis to help them make better decisions about their equity market investments. \n",
    "\n",
    "A couple of previous studies have compared linear regression models to artificial neural networks (ANN) and showed that ANNs yielded a much higher profit. The main difference in this observation is that ANNs can identify non-linear patterns. The chaotic and non-linear nature of the stock markets makes ANNs more popular for predictions. \n",
    "\n",
    "For this project, we've decided to investigate various ANN models and compare them to see which has the best performance. In particular, we will look at multi-layer perceptron and long short-term memory models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5581c2ad",
   "metadata": {},
   "source": [
    "#### Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93fc24c",
   "metadata": {},
   "source": [
    "LSTM networks:\n",
    "\n",
    "Before we build our LSTM network, we need to explore recurrent neural networks (RNN). LSTM networks are an extension built to solve one of the limitations of RNNs. \n",
    "\n",
    "An RNN functions similarly to the way a human does. Humans learn new things based on their previous knowledge. They don't think from scratch in every instance. Using this example, RNNs address this memory issue by storing information about the past. This RNN feature makes it useful for stock market predictions. In technical analysis of stocks, we use past information on stocks like their opening, closing prices, volumes etc. In an RNN model, we can take information at time $t-1$ and concatenate it to our input at time $t$ to predict the stock's price at time $t+1$. In figure [NO], we've illustrated the structure of the RNN we've built. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6610676b",
   "metadata": {},
   "source": [
    "However, the limitation of RNNs is their vanishing gradient problem. As explained earlier, RNNs retain all information from the past. If we visualise this, every single neuron in the model has contributed to the output. When the model is updated, it would have to propagate back through all these neurons. The problem arises here when the model updates the weights. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ec701f",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea7d9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1963c095",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7016900d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
